data:
  train_files: /workdir/project-search/rl4search/data/countdown/train.parquet
  val_files: /workdir/project-search/rl4search/data/countdown/test.parquet
  prompt_key: prompt
  max_prompt_length: 2048
  max_response_length: 2048
  train_batch_size: 1024

actor_rollout_ref:
 hybrid_engine: True
 model:
   path: /workdir/pretrained-models/Qwen-Qwen2-5-7B
   external_lib: null
   override_config: { }
   enable_gradient_checkpointing: False
   use_remove_padding: False
 actor:
  #  strategy: fsdp  # This is for backward-compatibility
   ppo_mini_batch_size: 128
   ppo_micro_batch_size: null # will be deprecated, use ppo_micro_batch_size_per_gpu
   ppo_micro_batch_size_per_gpu: 8
   use_dynamic_bsz: False
   ppo_max_token_len_per_gpu: 32768 # n * ${data.max_prompt_length} + ${data.max_response_length}
  #  grad_clip: 1.0
  #  clip_ratio: 0.2
  #  entropy_coeff: 0.001
   use_kl_loss: True # True for GRPO
   kl_loss_coef: 0.001 # for grpo
   kl_loss_type: low_var_kl # for grpo
  #  ppo_epochs: 1
   shuffle: False
  #  ulysses_sequence_parallel_size: 1 # sp size
   optim:
     lr: 1e-6
     lr_warmup_steps_ratio: 0.1  # the total steps will be injected during runtime
     min_lr_ratio: null   # only useful for warmup with cosine
     warmup_style: constant  # select from constant/cosine
    #  total_training_steps: -1  # must be override by program
  #  fsdp_config:
  #    wrap_policy:
  #     #  transformer_layer_cls_to_wrap: None
  #      min_num_params: 0
  #    param_offload: False
  #    optimizer_offload: False
  #    fsdp_size: -1
 ref:
   fsdp_config:
     param_offload: False
     wrap_policy:
       # transformer_layer_cls_to_wrap: None
       min_num_params: 0
   log_prob_micro_batch_size: null # will be deprecated, use log_prob_micro_batch_size_per_gpu
   log_prob_micro_batch_size_per_gpu: 16
   log_prob_use_dynamic_bsz: ${actor_rollout_ref.actor.use_dynamic_bsz}
   log_prob_max_token_len_per_gpu: ${actor_rollout_ref.actor.ppo_max_token_len_per_gpu}
   ulysses_sequence_parallel_size: ${actor_rollout_ref.actor.ulysses_sequence_parallel_size} # sp size
 rollout:
   name: vllm
   temperature: 0.7
   top_k: -1 # 0 for hf rollout, -1 for vllm rollout
   top_p: 1
   prompt_length: ${data.max_prompt_length}  # not use for opensource
   response_length: ${data.max_response_length}
   # for vllm rollout
   dtype: bfloat16 # should align with FSDP
   gpu_memory_utilization: 0.5
   ignore_eos: False
   enforce_eager: True
   free_cache_engine: True
   load_format: dummy_dtensor
   tensor_model_parallel_size: 2
   max_num_batched_tokens: 8192
   max_num_seqs: 1024
   log_prob_micro_batch_size: null # will be deprecated, use log_prob_micro_batch_size_per_gpu
   log_prob_micro_batch_size_per_gpu: 16
   log_prob_use_dynamic_bsz: ${actor_rollout_ref.actor.use_dynamic_bsz}
   log_prob_max_token_len_per_gpu: ${actor_rollout_ref.actor.ppo_max_token_len_per_gpu}
   # for hf rollout
   do_sample: True
   # number of responses (i.e. num sample times)
   n: 1 # > 1 for grpo, rloo
custom_reward_function:
  path: null
  name: compute_score
algorithm:
  adv_estimator: grpo
  gamma: 1.0
  lam: 1.0
  kl_penalty: kl  # how to estimate kl divergence
  kl_ctrl:
    type: fixed
    kl_coef: 0.005
trainer:
  total_epochs: 15
  project_name: verl_examples
  experiment_name: countdown
  logger: ['console', 'wandb']
  nnodes: 1
  n_gpus_per_node: 8
  save_freq: 10
  test_freq: 2
  # critic_warmup: 0
  default_hdfs_dir: ~/experiments/countdown/ppo/${trainer.experiment_name} # hdfs checkpoint path
  default_local_dir: /workdir/project-search/rl4search/checkpoints/${trainer.project_name}/${trainer.experiment_name} # local checkpoint path